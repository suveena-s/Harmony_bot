{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2617192,"sourceType":"datasetVersion","datasetId":1590810}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-04T06:01:26.421644Z","iopub.execute_input":"2024-01-04T06:01:26.422094Z","iopub.status.idle":"2024-01-04T06:01:26.838730Z","shell.execute_reply.started":"2024-01-04T06:01:26.422059Z","shell.execute_reply":"2024-01-04T06:01:26.837105Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/emotion-dataset/validation.csv\n/kaggle/input/emotion-dataset/training.csv\n/kaggle/input/emotion-dataset/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-12-30T18:31:08.174362Z","iopub.execute_input":"2023-12-30T18:31:08.175375Z","iopub.status.idle":"2023-12-30T18:31:08.180833Z","shell.execute_reply.started":"2023-12-30T18:31:08.175323Z","shell.execute_reply":"2023-12-30T18:31:08.179703Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"**BERT EMBEDDINGS**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import BertTokenizer, BertModel\nimport pandas as pd\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertModel.from_pretrained(model_name)\n\n\ndf = pd.read_csv('/kaggle/input/emotion-dataset/training.csv')\n\n# Tokenize and generate embeddings for each text in the dataset\nembeddings = []\nfor text in df['text']:\n    # Tokenize text\n    encoded_input = tokenizer(text, return_tensors='pt')\n    \n    # Generate embeddings\n    with torch.no_grad():\n        output = model(**encoded_input)\n    \n    # Extract embeddings from BERT's output\n    last_hidden_states = output.last_hidden_state\n    sentence_embedding = torch.mean(last_hidden_states, dim=1).squeeze().numpy()\n    embeddings.append(sentence_embedding)\n\n# Add embeddings to the DataFrame\ndf['embeddings'] = embeddings\n\n# Now df contains the original text, labels, and corresponding BERT embeddings\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T17:28:29.892928Z","iopub.execute_input":"2024-01-04T17:28:29.893342Z","iopub.status.idle":"2024-01-04T17:46:58.889977Z","shell.execute_reply.started":"2024-01-04T17:28:29.893313Z","shell.execute_reply":"2024-01-04T17:46:58.888836Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44bb612bf9844b7d884a484525e129de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc3b71b23aca4c94a6058416223bcf1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4da0cdde306437086782ad3499d3abb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f9141fa92c745e0b25296ddfd228d3e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8dc461653b841f89441fd81f3ac147c"}},"metadata":{}},{"name":"stdout","text":"                                                    text  label  \\\n0                                i didnt feel humiliated      0   \n1      i can go from feeling so hopeless to so damned...      0   \n2       im grabbing a minute to post i feel greedy wrong      3   \n3      i am ever feeling nostalgic about the fireplac...      2   \n4                                   i am feeling grouchy      3   \n...                                                  ...    ...   \n15995  i just had a very brief time in the beanbag an...      0   \n15996  i am now turning and i feel pathetic that i am...      0   \n15997                     i feel strong and good overall      1   \n15998  i feel like this was such a rude comment and i...      3   \n15999  i know a lot but i feel so stupid because i ca...      0   \n\n                                              embeddings  \n0      [-0.029084358, 0.28403538, -0.15271895, 0.2054...  \n1      [0.06209593, 0.35217133, 0.1729311, -0.0792969...  \n2      [0.44962013, 0.075376295, 0.44450906, 0.200843...  \n3      [-0.20407939, 0.12129271, 0.49728298, -0.29335...  \n4      [-0.26723722, 0.40239012, 0.106732205, -0.2897...  \n...                                                  ...  \n15995  [0.03693005, 0.13965839, -0.011357873, -0.1576...  \n15996  [0.09091516, 0.38120884, 0.35357383, -0.190214...  \n15997  [0.16753201, -0.011338789, 0.15695532, -0.1350...  \n15998  [-0.122115344, 0.2569666, 0.22916904, -0.12505...  \n15999  [0.111806795, 0.39334902, 0.24906981, -0.02861...  \n\n[16000 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**SVC on  BERT**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Assuming df contains the embeddings and labels\n\n# Split data into features (embeddings) and labels\nX = df['embeddings'].to_list()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize SVM classifier\nsvm = SVC(kernel='linear', C=1.0, random_state=42)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = svm.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:28:52.567483Z","iopub.execute_input":"2023-12-30T14:28:52.567839Z","iopub.status.idle":"2023-12-30T14:30:53.148667Z","shell.execute_reply.started":"2023-12-30T14:28:52.567809Z","shell.execute_reply":"2023-12-30T14:30:53.147297Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Accuracy: 0.62\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.62      0.73      0.67       946\n           1       0.68      0.76      0.72      1021\n           2       0.54      0.34      0.42       296\n           3       0.58      0.48      0.53       427\n           4       0.59      0.49      0.53       397\n           5       0.44      0.27      0.34       113\n\n    accuracy                           0.62      3200\n   macro avg       0.57      0.51      0.53      3200\nweighted avg       0.62      0.62      0.61      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**RFC on BERT**","metadata":{}},{"cell_type":"code","source":"#from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import accuracy_score, classification_report\n\n# Assuming df contains the embeddings and labels\n\n# Split data into features (embeddings) and labels\nX = df['embeddings'].to_list()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize Random Forest classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the Random Forest classifier\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T14:30:53.151268Z","iopub.execute_input":"2023-12-30T14:30:53.151623Z","iopub.status.idle":"2023-12-30T14:31:34.751711Z","shell.execute_reply.started":"2023-12-30T14:30:53.151592Z","shell.execute_reply":"2023-12-30T14:31:34.750708Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Accuracy: 0.52\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.49      0.75      0.59       946\n           1       0.54      0.87      0.67      1021\n           2       0.00      0.00      0.00       296\n           3       0.67      0.11      0.19       427\n           4       0.50      0.05      0.09       397\n           5       0.00      0.00      0.00       113\n\n    accuracy                           0.52      3200\n   macro avg       0.37      0.30      0.26      3200\nweighted avg       0.47      0.52      0.42      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**roberta embeddings**","metadata":{}},{"cell_type":"code","source":"#import torch\nfrom transformers import RobertaModel, RobertaTokenizer\n#import pandas as pd\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = 'roberta-base'  # You can use different variations of RoBERTa if needed\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaModel.from_pretrained(model_name)\n\n\n# Tokenize and obtain RoBERTa embeddings\ndef get_roberta_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n    return embeddings\n\n# Apply the function to your dataset\nembeddings_list = []\nfor row in df['text']:\n    embedding = get_roberta_embeddings(row)\n    embeddings_list.append(embedding)\n\n# Concatenate the embeddings and add them as new columns in your dataset\nembeddings_tensor = torch.cat(embeddings_list)\nembeddings_df = pd.DataFrame(embeddings_tensor.numpy())\n\n# Merge the original dataframe with the embeddings dataframe\nresult_df = pd.concat([df, embeddings_df], axis=1)\nprint(result_df)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**roberta emd**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import RobertaModel, RobertaTokenizer\nimport pandas as pd\n\n# Load pre-trained RoBERTa model and tokenizer\nmodel_name = 'roberta-base'  # You can use different variations of RoBERTa if needed\ntokenizer = RobertaTokenizer.from_pretrained(model_name)\nmodel = RobertaModel.from_pretrained(model_name)\n\n# Your existing DataFrame with 'text' column\n# Assuming df contains the text data\n# ...\n\n# Tokenize and obtain RoBERTa embeddings\ndef get_roberta_embeddings(text):\n    inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling of token embeddings\n    return embeddings\n\n# Apply the function to your dataset and store embeddings in a new column 'roberta_emb'\ndf['roberta_emb'] = df['text'].apply(lambda x: get_roberta_embeddings(x)[0].numpy())\n\n# Display the DataFrame with the new 'roberta_emb' column containing embeddings\nprint(df)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-31T09:14:37.411998Z","iopub.execute_input":"2023-12-31T09:14:37.412395Z","iopub.status.idle":"2023-12-31T09:37:43.586666Z","shell.execute_reply.started":"2023-12-31T09:14:37.412360Z","shell.execute_reply":"2023-12-31T09:37:43.585393Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4620278a3042ab8ac9febd979e1167"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a391486f95472f886626483484ee30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"528b45127ec341a7a791e5158acf818f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c2744688bf64911a159db8cb6df1841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d16ab5aa63c045028c9f81562e02348b"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"                                                    text  label  \\\n0                                i didnt feel humiliated      0   \n1      i can go from feeling so hopeless to so damned...      0   \n2       im grabbing a minute to post i feel greedy wrong      3   \n3      i am ever feeling nostalgic about the fireplac...      2   \n4                                   i am feeling grouchy      3   \n...                                                  ...    ...   \n15995  i just had a very brief time in the beanbag an...      0   \n15996  i am now turning and i feel pathetic that i am...      0   \n15997                     i feel strong and good overall      1   \n15998  i feel like this was such a rude comment and i...      3   \n15999  i know a lot but i feel so stupid because i ca...      0   \n\n                                              embeddings  \\\n0      [-0.029084232, 0.28403535, -0.15271899, 0.2054...   \n1      [0.062095962, 0.35217127, 0.1729311, -0.079297...   \n2      [0.44961998, 0.075376205, 0.4445088, 0.2008439...   \n3      [-0.2040793, 0.12129233, 0.49728298, -0.293353...   \n4      [-0.267237, 0.40239015, 0.106732294, -0.289782...   \n...                                                  ...   \n15995  [0.036930032, 0.13965842, -0.011358027, -0.157...   \n15996  [0.09091488, 0.3812088, 0.35357383, -0.1902150...   \n15997  [0.16753188, -0.0113387965, 0.1569557, -0.1350...   \n15998  [-0.122115344, 0.25696644, 0.22916913, -0.1250...   \n15999  [0.11180659, 0.3933487, 0.24906981, -0.0286107...   \n\n                                             roberta_emb  \n0      [-0.029875472, -0.12261019, -0.075354286, -0.1...  \n1      [0.027807666, 0.08506984, 0.08304222, -0.19231...  \n2      [-0.018578397, -0.13414888, -0.018878791, -0.1...  \n3      [0.08331484, 0.018418241, -0.020032551, -0.122...  \n4      [0.001202818, 0.06340924, 0.03927246, -0.13768...  \n...                                                  ...  \n15995  [0.042974286, -0.06786098, 0.03850568, -0.0576...  \n15996  [0.062157393, -0.03378462, 0.0077818087, 0.015...  \n15997  [-0.03378327, 0.0037925518, 0.053695656, -0.13...  \n15998  [0.024002088, -0.024004865, -0.011801992, -0.0...  \n15999  [0.081347406, -0.14630297, -0.012619911, -0.09...  \n\n[16000 rows x 4 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**SVC**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split the data into features (embeddings) and labels\nX = df['roberta_emb'].tolist()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize SVM classifier\nsvm = SVC(kernel='linear', C=1.0, random_state=42)\n\n# Train the SVM classifier\nsvm.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = svm.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:10:06.452920Z","iopub.execute_input":"2023-12-30T15:10:06.453290Z","iopub.status.idle":"2023-12-30T15:11:37.497744Z","shell.execute_reply.started":"2023-12-30T15:10:06.453255Z","shell.execute_reply":"2023-12-30T15:11:37.496519Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Accuracy: 0.66\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.65      0.75      0.70       946\n           1       0.70      0.81      0.75      1021\n           2       0.62      0.34      0.44       296\n           3       0.62      0.53      0.57       427\n           4       0.64      0.52      0.57       397\n           5       0.53      0.35      0.42       113\n\n    accuracy                           0.66      3200\n   macro avg       0.63      0.55      0.58      3200\nweighted avg       0.65      0.66      0.65      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**RFC**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Split the data into features (embeddings) and labels\nX = df['roberta_emb'].tolist()  # Features (embeddings)\ny = df['label']  # Labels\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize Random Forest classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the Random Forest classifier\nrf.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = rf.predict(X_test)\n\n# Evaluate the classifier\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Accuracy: {accuracy:.2f}\")\n\n# Print classification report\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:11:37.499182Z","iopub.execute_input":"2023-12-30T15:11:37.499672Z","iopub.status.idle":"2023-12-30T15:12:19.435286Z","shell.execute_reply.started":"2023-12-30T15:11:37.499636Z","shell.execute_reply":"2023-12-30T15:12:19.434036Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Accuracy: 0.51\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.48      0.74      0.58       946\n           1       0.52      0.86      0.65      1021\n           2       0.00      0.00      0.00       296\n           3       0.68      0.07      0.12       427\n           4       0.85      0.06      0.10       397\n           5       0.00      0.00      0.00       113\n\n    accuracy                           0.51      3200\n   macro avg       0.42      0.29      0.24      3200\nweighted avg       0.50      0.51      0.41      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**ANN on roberta**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Convert labels to numerical values using LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['encoded_labels'] = label_encoder.fit_transform(df['label'])\n\n# Split the data into features (embeddings) and encoded labels\nX = df['roberta_emb'].tolist()  # Features (embeddings)\ny = df['encoded_labels']  # Encoded Labels\n\n# Convert to PyTorch tensors\nX = torch.tensor(X)\ny = torch.tensor(y)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the neural network architecture\nclass ANN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ANN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# Set input, hidden, and output sizes\ninput_size = len(X[0])\nhidden_size = 128  # Adjust the hidden layer size as needed\noutput_size = len(label_encoder.classes_)\n\n# Initialize the neural network model\nmodel = ANN(input_size, hidden_size, output_size)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n\n# Training the model\nnum_epochs = 300  # Adjust the number of epochs as needed\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_train.float())\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluation on test data\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test.float())\n    _, predicted = torch.max(outputs, 1)\n    accuracy = accuracy_score(y_test, predicted)\n    print(f'Accuracy: {accuracy:.2f}')\n    print('Classification Report:')\n    print(classification_report(y_test, predicted))\n","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:12:19.436716Z","iopub.execute_input":"2023-12-30T15:12:19.437059Z","iopub.status.idle":"2023-12-30T15:12:35.914285Z","shell.execute_reply.started":"2023-12-30T15:12:19.437023Z","shell.execute_reply":"2023-12-30T15:12:35.913531Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Epoch [10/300], Loss: 1.5469\nEpoch [20/300], Loss: 1.4947\nEpoch [30/300], Loss: 1.4298\nEpoch [40/300], Loss: 1.3619\nEpoch [50/300], Loss: 1.2945\nEpoch [60/300], Loss: 1.2341\nEpoch [70/300], Loss: 1.1827\nEpoch [80/300], Loss: 1.1385\nEpoch [90/300], Loss: 1.1000\nEpoch [100/300], Loss: 1.0663\nEpoch [110/300], Loss: 1.0368\nEpoch [120/300], Loss: 1.0108\nEpoch [130/300], Loss: 0.9879\nEpoch [140/300], Loss: 0.9675\nEpoch [150/300], Loss: 0.9492\nEpoch [160/300], Loss: 0.9328\nEpoch [170/300], Loss: 0.9180\nEpoch [180/300], Loss: 0.9048\nEpoch [190/300], Loss: 0.8928\nEpoch [200/300], Loss: 0.8820\nEpoch [210/300], Loss: 0.8721\nEpoch [220/300], Loss: 0.8629\nEpoch [230/300], Loss: 0.8544\nEpoch [240/300], Loss: 0.8464\nEpoch [250/300], Loss: 0.8389\nEpoch [260/300], Loss: 0.8317\nEpoch [270/300], Loss: 0.8249\nEpoch [280/300], Loss: 0.8184\nEpoch [290/300], Loss: 0.8121\nEpoch [300/300], Loss: 0.8060\nAccuracy: 0.65\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.66      0.74      0.70       946\n           1       0.69      0.81      0.75      1021\n           2       0.58      0.32      0.41       296\n           3       0.61      0.52      0.56       427\n           4       0.61      0.53      0.57       397\n           5       0.52      0.30      0.38       113\n\n    accuracy                           0.65      3200\n   macro avg       0.61      0.54      0.56      3200\nweighted avg       0.64      0.65      0.64      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**ANN on bert**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Convert labels to numerical values using LabelEncoder\nlabel_encoder = LabelEncoder()\ndf['encoded_labels'] = label_encoder.fit_transform(df['label'])\n\n# Split the data into features (embeddings) and encoded labels\nX = df['embeddings'].tolist()  # Features (embeddings)\ny = df['encoded_labels']  # Encoded Labels\n\n# Convert to PyTorch tensors\nX = torch.tensor(X)\ny = torch.tensor(y)\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Define the neural network architecture\nclass ANN(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(ANN, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, output_size)\n    \n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.relu(out)\n        out = self.fc2(out)\n        return out\n\n# Set input, hidden, and output sizes\ninput_size = len(X[0])\nhidden_size = 128  # Adjust the hidden layer size as needed\noutput_size = len(label_encoder.classes_)\n\n# Initialize the neural network model\nmodel = ANN(input_size, hidden_size, output_size)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)  # Adjust learning rate as needed\n\n# Training the model\nnum_epochs = 300  # Adjust the number of epochs as needed\nfor epoch in range(num_epochs):\n    optimizer.zero_grad()\n    outputs = model(X_train.float())\n    loss = criterion(outputs, y_train)\n    loss.backward()\n    optimizer.step()\n    if (epoch+1) % 10 == 0:\n        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n\n# Evaluation on test data\nwith torch.no_grad():\n    model.eval()\n    outputs = model(X_test.float())\n    _, predicted = torch.max(outputs, 1)\n    accuracy = accuracy_score(y_test, predicted)\n    print(f'Accuracy: {accuracy:.2f}')\n    print('Classification Report:')\n    print(classification_report(y_test, predicted))","metadata":{"execution":{"iopub.status.busy":"2023-12-30T15:12:35.915162Z","iopub.execute_input":"2023-12-30T15:12:35.915456Z","iopub.status.idle":"2023-12-30T15:12:52.887654Z","shell.execute_reply.started":"2023-12-30T15:12:35.915430Z","shell.execute_reply":"2023-12-30T15:12:52.886572Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Epoch [10/300], Loss: 1.4214\nEpoch [20/300], Loss: 1.2776\nEpoch [30/300], Loss: 1.1884\nEpoch [40/300], Loss: 1.1268\nEpoch [50/300], Loss: 1.0794\nEpoch [60/300], Loss: 1.0416\nEpoch [70/300], Loss: 1.0099\nEpoch [80/300], Loss: 0.9825\nEpoch [90/300], Loss: 0.9588\nEpoch [100/300], Loss: 0.9383\nEpoch [110/300], Loss: 0.9201\nEpoch [120/300], Loss: 0.9038\nEpoch [130/300], Loss: 0.8887\nEpoch [140/300], Loss: 0.8746\nEpoch [150/300], Loss: 0.8613\nEpoch [160/300], Loss: 0.8486\nEpoch [170/300], Loss: 0.8366\nEpoch [180/300], Loss: 0.8250\nEpoch [190/300], Loss: 0.8139\nEpoch [200/300], Loss: 0.8030\nEpoch [210/300], Loss: 0.7926\nEpoch [220/300], Loss: 0.7826\nEpoch [230/300], Loss: 0.7727\nEpoch [240/300], Loss: 0.7631\nEpoch [250/300], Loss: 0.7534\nEpoch [260/300], Loss: 0.7436\nEpoch [270/300], Loss: 0.7340\nEpoch [280/300], Loss: 0.7248\nEpoch [290/300], Loss: 0.7156\nEpoch [300/300], Loss: 0.7065\nAccuracy: 0.64\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.65      0.70      0.68       946\n           1       0.68      0.80      0.74      1021\n           2       0.52      0.30      0.38       296\n           3       0.59      0.53      0.56       427\n           4       0.58      0.54      0.56       397\n           5       0.45      0.26      0.33       113\n\n    accuracy                           0.64      3200\n   macro avg       0.58      0.52      0.54      3200\nweighted avg       0.63      0.64      0.63      3200\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**classification using bert**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\ndf=pd.read_csv('/kaggle/input/emotion-dataset/training.csv')\n\n# Load pre-trained BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\ntokenizer = BertTokenizer.from_pretrained(model_name)\nmodel = BertForSequenceClassification.from_pretrained(model_name, num_labels=6)  # num_classes is the number of unique classes in your dataset\n\n# Assuming df contains the BERT embeddings and labels\nX = df['text'].tolist()  # Text data\ny = df['label']  # Target\n\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Tokenize text after splitting the data\ntokenized_train = tokenizer(X_train, padding=True, truncation=True, return_tensors='pt')\ntokenized_test = tokenizer(X_test, padding=True, truncation=True, return_tensors='pt')\n\n# Convert y_train to tensor\ny_train = torch.tensor(y_train.values.astype(np.int64))  # Convert y_train to a tensor\n\n\n# Create data loaders\ntrain_data = torch.utils.data.TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(y_train))\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=20)\n\n# Define training parameters\n#batch_size = 16\nepochs = 3\nlearning_rate = 2e-5\n\n\n\n# Set optimizer and loss function\noptimizer = AdamW(model.parameters(), lr=learning_rate)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Training loop\nfor epoch in range(epochs):\n    model.train()\n    total_loss = 0\n    \n    for batch in train_loader:\n        optimizer.zero_grad()\n        input_ids, attention_mask, labels = batch\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n    \n    avg_train_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{epochs} - Average Training Loss: {avg_train_loss:.4f}\")\n\n# Evaluate the model\nmodel.eval()\ntest_input_ids = X_test['input_ids']\ntest_attention_mask = X_test['attention_mask']\nwith torch.no_grad():\n    outputs = model(test_input_ids, attention_mask=test_attention_mask)\n    logits = outputs.logits\n    predictions = torch.argmax(logits, dim=1)\n\n# Calculate accuracy\naccuracy = accuracy_score(y_test, predictions)\nprint(f\"Accuracy on test set: {accuracy * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-04T14:58:18.022950Z","iopub.execute_input":"2024-01-04T14:58:18.023317Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_42/1438899797.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  train_data = torch.utils.data.TensorDataset(tokenized_train['input_ids'], tokenized_train['attention_mask'], torch.tensor(y_train))\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3 - Average Training Loss: 0.5725\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**distilbert**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load dataset\ndf = pd.read_csv('/kaggle/input/emotion-dataset/training.csv')  \n\n# Display the first few rows to check the data\n\nfrom sklearn.model_selection import train_test_split\nfrom transformers import DistilBertTokenizerFast\n\n# Split the data into training and validation sets\ntrain_texts, test_texts, train_labels, test_labels = train_test_split(\n    df['text'], df['label'], test_size=0.2, random_state=42\n)\n\n# Initialize DistilBERT tokenizer\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n\n# Tokenize the text data\ntrain_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nfrom sklearn.preprocessing import LabelEncoder\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit and transform labels for training data\ntrain_labels_encoded = label_encoder.fit_transform(train_labels)\n\n# Transform labels for validation data\ntest_labels_encoded = label_encoder.transform(test_labels)\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\n# Create PyTorch Datasets\ntrain_dataset = CustomDataset(train_encodings, train_labels_encoded)\ntest_dataset = CustomDataset(test_encodings, test_labels_encoded)\n\n# Create DataLoaders\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=16)\n\nfrom transformers import DistilBertForSequenceClassification, AdamW\nfrom tqdm import tqdm\n\n# Initialize the model\nmodel = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)\n\n# Define optimizer and learning rate scheduler\noptimizer = AdamW(model.parameters(), lr=5e-5)\n\n# Training loop\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\nmodel.train()\n\nfor epoch in range(3):  # Set your desired number of epochs\n    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n        optimizer.zero_grad()\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n# Evaluation\nmodel.eval()\ntest_preds = []\ntest_true = []\n\nfor batch in tqdm(test_loader):\n    with torch.no_grad():\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask)\n        preds = torch.argmax(outputs.logits, dim=1)\n        test_preds.extend(preds.cpu().detach().numpy())\n        test_true.extend(labels.cpu().detach().numpy())\n\n# Calculate accuracy\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(test_true, test_preds)\nprint(f\"test Accuracy: {accuracy}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:13:35.145562Z","iopub.execute_input":"2024-01-08T14:13:35.146594Z","iopub.status.idle":"2024-01-08T14:18:37.398968Z","shell.execute_reply.started":"2024-01-08T14:13:35.146559Z","shell.execute_reply":"2024-01-08T14:18:37.398013Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'pre_classifier.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\nEpoch 1: 100%|██████████| 800/800 [01:38<00:00,  8.15it/s]\nEpoch 2: 100%|██████████| 800/800 [01:37<00:00,  8.18it/s]\nEpoch 3: 100%|██████████| 800/800 [01:37<00:00,  8.20it/s]\n100%|██████████| 200/200 [00:06<00:00, 29.55it/s]","output_type":"stream"},{"name":"stdout","text":"test Accuracy: 0.9290625\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"import pickle","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:19:00.765624Z","iopub.execute_input":"2024-01-08T14:19:00.766011Z","iopub.status.idle":"2024-01-08T14:19:00.770601Z","shell.execute_reply.started":"2024-01-08T14:19:00.765982Z","shell.execute_reply":"2024-01-08T14:19:00.769521Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"pickle.dump(model,open('/kaggle/working/modelsaved','wb'))","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:24:53.292938Z","iopub.execute_input":"2024-01-08T14:24:53.293375Z","iopub.status.idle":"2024-01-08T14:24:53.896826Z","shell.execute_reply.started":"2024-01-08T14:24:53.293342Z","shell.execute_reply":"2024-01-08T14:24:53.895954Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import joblib","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:19:28.031927Z","iopub.execute_input":"2024-01-08T14:19:28.032883Z","iopub.status.idle":"2024-01-08T14:19:28.037054Z","shell.execute_reply.started":"2024-01-08T14:19:28.032845Z","shell.execute_reply":"2024-01-08T14:19:28.035996Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"joblib.dump(model,'savedmodel')","metadata":{"execution":{"iopub.status.busy":"2024-01-08T14:22:22.133882Z","iopub.execute_input":"2024-01-08T14:22:22.134568Z","iopub.status.idle":"2024-01-08T14:22:22.792357Z","shell.execute_reply.started":"2024-01-08T14:22:22.134531Z","shell.execute_reply":"2024-01-08T14:22:22.791428Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"['savedmodel']"},"metadata":{}}]}]}